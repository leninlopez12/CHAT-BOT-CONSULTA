# -*- coding: utf-8 -*-
"""chat bot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b2A1Zw5tGAw26Sschei4vpoYZnomlaag
"""

# Elasticsearch
!pip install elasticsearch

# Iniciar Elasticsearch
import subprocess


# Langchain
!pip install langchain

# Gradio
!pip install gradio

# Python
import os
import gradio as gr
from langchain import OpenAI
from elasticsearch import Elasticsearch, helpers

# Tu código...

# Detener elasticsearch al final

!pip install transformers
import transformers
import gradio as gr
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the pre-trained GPT-2 model and tokenizer
model_name = "gpt2"  # You can also try "gpt2-medium" or "gpt2-large" for larger models
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Load documents from the file
def load_documents(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return [line.strip() for line in file]

# Load the training documents
file_path = "entrenamiento.txt"
docs = load_documents(file_path)

def chatbot(input_text):
    # Encode the input text
    input_ids = tokenizer.encode(input_text, return_tensors="pt")

    # Generate a response using the GPT-2 model
    with torch.no_grad():
        output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)

    # Decode the response
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

app = gr.Interface(fn=chatbot,
                   inputs=gr.inputs.Textbox(lines=5, label="ingresa una petición"),
                   outputs="text",
                   title="Chatbot")

app.launch(share=False)

import torch

import torch
import gradio as gr
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the pre-trained GPT-2 model and tokenizer
model_name = "gpt2"  # You can also try "gpt2-medium" or "gpt2-large" for larger models
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Load documents from the file
def load_documents(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return [line.strip() for line in file]

# Load the training documents
file_path = "entrenamiento.txt"
docs = load_documents(file_path)

def chatbot(input_text):
    # Encode the input text
    input_ids = tokenizer.encode(input_text, return_tensors="pt")

    # Generate a response using the GPT-2 model
    with torch.no_grad():
        output = model.generate(input_ids,
                                max_length=100,
                                num_return_sequences=1,
                                pad_token_id=tokenizer.eos_token_id,
                                do_sample=True,   # Set to True for response generation
                                top_k=50,        # Control the diversity of generated responses
                                temperature=0.7) # Control the randomness of generated responses

    # Decode the response
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

app = gr.Interface(fn=chatbot,
                   inputs=gr.inputs.Textbox(lines=5, label="ingresa una petición"),
                   outputs="text",
                   title="Chatbot")

app.launch(share=False)

import gradio as gr
from transformers import BartTokenizer, BartForConditionalGeneration

# Load the pre-trained BART model and tokenizer
model_name = "facebook/bart-large"  # You can try other BART variants if needed
model = BartForConditionalGeneration.from_pretrained(model_name)
tokenizer = BartTokenizer.from_pretrained(model_name)

# Rest of your code...


# Load documents from the file
def load_documents(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return [line.strip() for line in file]

# Load the training documents from "entrenamiento.txt"
file_path = "entrenamiento.txt"
training_data = load_documents(file_path)

# Fine-tune the model on the training dataset
for example in training_data:
    input_text = "context: " + example + " </s>"
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)

    model.train()
    model(input_ids=input_ids)

# Chatbot function using the fine-tuned T5 model
def chatbot(input_text):
    input_text = "context: " + input_text + " </s>"
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)

    model.eval()
    with torch.no_grad():
        output = model.generate(input_ids, max_length=100, num_beams=4, early_stopping=True, num_return_sequences=1)

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

# Create and launch the Gradio interface
app = gr.Interface(fn=chatbot,
                   inputs=gr.inputs.Textbox(lines=5, label="ingresa una petición"),
                   outputs="text",
                   title="Chatbot")

app.launch(share=False)

import gradio as gr
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load the pre-trained GPT-2 model and tokenizer
model_name = "gpt2"  # You can try "gpt2-medium" or "gpt2-large" for larger models
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Load documents from the file
def load_documents(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return [line.strip() for line in file]

# Load the training documents from "entrenamiento.txt"
file_path = "entrenamiento.txt"
training_data = load_documents(file_path)

# Combine the training data and some predefined responses for the chatbot
chatbot_responses = training_data + [
    "El comedor universitario solo atiende en el almuerzo; no tiene desayuno ni cena.",
    "Para tener acceso al comedor universitario, es requisito indispensable estar en extrema pobreza y vivir en provincia y distritos pobres.",
    "Los pasos para obtener tu bachiller son: terminar tu carrera, no deber nada a la Universidad, presentar el certificado de prácticas preprofesionales y tu ficha de matrícula."
]

def chatbot(input_text):
    input_text = "User: " + input_text + " "
    input_ids = tokenizer.encode(input_text, return_tensors="pt")

    model.eval()
    with torch.no_grad():
        output = model.generate(input_ids, max_length=100, num_beams=4, early_stopping=True, num_return_sequences=1)

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

app = gr.Interface(fn=chatbot,
                   inputs=gr.inputs.Textbox(lines=5, label="ingresa una petición"),
                   outputs="text",
                   title="Chatbot")

app.launch(share=True)

"""CLAUDE CODIGO

"""

import torch
import gradio as gr
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Cargar el modelo GPT-2 entrenado
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Cargar el tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Definir la función para responder
def responde(pregunta):

  # Codificar la pregunta
  inputs = tokenizer(pregunta, return_tensors="pt")

  # Generar respuestas con el modelo
  outputs = model.generate(inputs["input_ids"],
                           max_length=150,
                           do_sample=True,
                           top_p=0.95,
                           top_k=50,
                           temperature=0.8)

  # Decodificar la respuesta
  texto = tokenizer.decode(outputs[0], skip_special_tokens=True)

  return texto

# Interfaz con Gradio
interfaz = gr.Interface(fn=responde,
                        inputs="text",
                        outputs="text",
                        title="GPT-2 Chatbot",
                        description="Hazme una pregunta sobre comedor universitario o trámite de bachiller")

interfaz.launch()

import torch
import gradio as gr
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the pre-trained GPT-2 model and tokenizer
model_name = "gpt2"  # You can also try "gpt2-medium" or "gpt2-large" for larger models
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Load documents from the file
def load_documents(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return [line.strip() for line in file]

# Load the training documents
file_path = "datos1.txt"
docs = load_documents(file_path)

def chatbot(input_text):
    # Encode the input text
    input_ids = tokenizer.encode(input_text, return_tensors="pt")

    # Generate a response using the GPT-2 model
    with torch.no_grad():
        output = model.generate(input_ids,
                                max_length=100,
                                num_return_sequences=1,
                                pad_token_id=tokenizer.eos_token_id,
                                do_sample=True,   # Set to True for response generation
                                top_k=50,        # Control the diversity of generated responses
                                temperature=0.7) # Control the randomness of generated responses

    # Decode the response
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

app = gr.Interface(fn=chatbot,
                   inputs=gr.inputs.Textbox(lines=5, label="ingresa una petición"),
                   outputs="text",
                   title="Chatbot")

app.launch(share=True)

docs